{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project to Predict Vehicle Decade, Body Style, and Make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Chris Haynes, Ben Newell, and Ryan Williams 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided it would be interesting to try and predict a vehicle's decade, the body style, and the make of the cars passed in from an image. To add to the ease of using the neural network and to satisfy an honors option, we also made a web application that allows the user to load a .JPG or .JPEG image of a car and a selection of a classification type to make it easier to use the neural networks. For the web application, we built the application in React.js, and hosted a server in Python Flask with a RESTful API to upload images and classify that image using REST principles. For the neural networks, the hardest part was working with the data. We had to select the right image size, aspect ratios, cropping to the body of the car, standardization of the data, and batch sizes. Choosing the right parameters for the neural networks helped as well.\n",
    "\n",
    "All the source code for this project is located in the following git repository: https://github.com/chrishaynes21/ocular-vehicular-patdown\n",
    "    \n",
    "The data used for this assigment can be found here: https://ai.stanford.edu/~jkrause/cars/car_dataset.html\n",
    "    \n",
    "Our submission on canvas will be a zipped file containing this notebook; training notebooks for vehicle make, body style and decade; and an image processing notebook. Additionally, the zipped file will contain the trained neural network objects and all the screenshots taken while testing how well the website classified automobiles  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Application - Chris\n",
    "Writing the web application was slightly challenging, especially passing the image from the website to the server. The first thing that I did was begin by running `npx create-react-app` to initialize a simple React web application. This is very basic and essentially handles the creation of files such as `index.js` and `App.js` with some simple React to display a generalized React splash page. It also handles the creation of the `node_modules` directory, which has `react-scripts` that can be run for different lifecycles of the application. The final thing is the `package.json`, which contains all packages that are necassary to run the application. Running `npm install` will install all the correct dependencies for the project without the need to download them one by one, as would be necassary with a `pip install`. This also references the `react-strap` node module and will run the scripts for certain commands. The most common command during development is `npm start` which starts the development server. The start script is not an optimized build, but is nice to use because it will hot load when a change in the React code occurs. Therefore, this command is usually only run once, and when tinkering with the website it prevents the need to make a change and restart the development server. On the actually Python server, we decided to use Python Flask, which is a web framework, but worked just fine providing the RESTful API to the React application. All of the server for the API is contained in `server.py`.\n",
    "\n",
    "To style the web application, we decided to use the node module called `react-strap`, which also means we needed to install `bootstrap` as well. Bootstrap is common library used for web applications and it's principles are widely known for user interfaces. It has layouts based on a theory of 12, that the layouts should be split into categories, so adding three columns could mean their widths would equally be 4 for a layout, or one large one at 6 and the other two at 3, etc. Not only does it handle the layouts well, but it also has great styling, so we did not need to spend much time working on CSS. It has a React class called `Jumbotron` that makes a nice page header, and a `Navbar` class that works for navigation. If there were more pages than just the one we used, these links would have been available in the nav bar of our web application.\n",
    "\n",
    "The class were we made the most happen was in `Loader.js`. This class handled the layout below the page header, and included all of the buttons and the image upload. We used some simple React principles to make this class work. We'll start with the buttons. To highlight the button that has been selected, we stored the state of the selected button in the Loader component. If a button was the `activeButton`, they would highlight from a `secondary` color to a `primary` color. We used one function to change the state of `activeButton` upon a click of the button. If a user has pressed the Submit button to actually submit the image to classify, the button will become disabled, since the type of the classification has already been chosen. \n",
    "\n",
    "The part that took the longest was getting the file to upload. We did this first by determining if the user was still `loading` an image and had not yet submitted an image. By default, the user starts with `loading` set to `true`. The nice thing is that `reactstrap` has a `Form` class with an `Input`. In the `Input` component, one can select the `type` of the upload, which is set to file. We do validation that it is an image on the server, so no validation was necassary on the client side. Using the `onChange` attribute, we pass the event to my `fileChanged` function. We use the `event.target.files[0]` to store the first file submitted and we display it on the page in a small preview. We achieved the display by using a conditional to display the image if the file was present. If it is not an image, it displays alternative text, but the file will not be accepted upon submitting the file. The magic all happens in `submitImage`, and we followed the blog post from [Ashish Pandey](https://medium.com/@ashishpandey_1612/file-upload-with-react-flask-e115e6f2bf99) to handle submission (Pandey). The submit button cannot be clicked until a file has been loaded, but `onClick` calls `submitImage`. First, it starts by creating `FormData` to pass to the server, and appends the file to that object. If a classification is not chosen, it will trigger an alert that a classification must be chosen. Otherwise, it `fetch` call to the server, which runs on `http://127.0.0.1:5000`. It calls the `upload` using the HTTP verb of `POST` to post the file to the server and appends the `FormData` object as the body of the call. On the server, we check to see if it is a legal file, and we throw an error if it is not legal. If it is legal, namely a JPEG or JPG image, we make a directory on the server called `uploads`. It saves the image to this directory and returns a JSON object containing the name of the file on the server. On the front end, we use the `.then` function to await the response from the server since a fetch is asynchronous and returns a promise. We check if there is an error and if no error is thrown, it will use the file name and call `getClassification` with the file name returned from the server. This function uses another `fetch` and a `GET` request to the `classification` API. It does this using a query string with the file name and the selected classification type. On the server, we get both the file name and classification type from the query string, and uses them to call the correct neural network and get the classification. We call the correct pickled neural network object, and call the `use` function to get the classification. We call the `get_classification` function to get the classification's name from our image processing file. This is then returned back to the web application and displayed on the web page in place of the file upload, with the image still displayed on the right. The final button one will see is a reset button, that allows for the image to be kept and a different classification displayed, or a new file to be uploaded and a classification attempted.\n",
    "\n",
    "#### To Run\n",
    "First, one must run an `npm install` to get the correct node packages. Then `npm start` is used to fire the developers server for the web application. From here, one must untar the `nnets` tar into a directory at `app\\server\\nnet\\`. If one wishes to run the server on something other than `127.0.0.1:5000`, the host IP address must be changed to the local machine's IP address in both `app\\server\\server.py` in `app.run(hostname=yourIP, port=whatever)` (line 81) and in `app\\static\\src\\Loader.js` (lines 39 & 65) to the correct ip and port provided in the app run for Flask. So there are three places to change this to the correct server IP. Running `app\\server\\server.py` should successfully launch the web server API. If the IP is changed in the server (for me I used `192.168.1.117:44100`) one could be using their phone or another computer on the local network can connect to the developer server for the web app provided from the `npm start` command (not the same as the Flask server with host and IP above, that's just to connect the web app to the API).\n",
    "\n",
    "#### Challenges\n",
    "The challenges we ran into was first of all getting the image to the server. We first tried using JSON objects to pass to the server, but getting the file was impossible as far as we could tell from debugging the request object. Using `FormData`proved much more successful. Another problem that we ran into was a warning that we were not using `CORS` requests between the server and client. By adding a `CORS` object to surround the server app, we were able to do cross origin requests without errors. We had to see how to parse the returned status that came back from the methods, and doing different statuses, but it proved easier to return JSON strings to front end to use. The challenge with the neural network objects is that they were trained on the GPU. We got several errors related to the fact that it was trained on the GPU rather than CPU. Changing the `map_location` didn't work for `torch.load`, so on CoLab, we call `nnet.to(torch.device('cpu')` and then set `nnet.device = 'cpu'`. This allowed for the pickled object to be loaded onto the CPU the server resides on, and to use that network to classify the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"Image-Processing\"></a>Image Processing and Data preparation - Ben\n",
    "\n",
    "   The data used in this experiment comes from the Stanford Cars dataset. These photos were stored in jpg format. Data processing had to be done in two ways on this data. First, we had to read in the metadata containing the photo’s name, car type, and bounding box. Then, the photos themselves had to be read in into a numpy format compatible with the neural network class. \n",
    "\n",
    "### Description of metadata\n",
    "\n",
    "   The metadata describing the images is presented in three .mat matlab files. The file `cars_meta.mat` contains class names for each of the classes of cars. So if the user wants to determine what car class 57 is, for example, they would use this file. The other two files, `cars_test_annos.mat` and `cars_train_annos.mat` contain information about every image file in the respective test and train folders. The folders `car_test` and `car_train` contain thousands of jpg images of cars. The file `car_train_annos.mat` has descriptions of bounding boxes and class names for each of the images in the `car_train` folder. The bounding box description is of X and Y coordinates of the rectangle enclosing the car in the image. This information is important for training the neural network. In order for it to have the best chance of picking up the trend, only relevant information should be shown. The class number, in combination with `cars_meta.mat` allows the user to find what car is shown in that image. Unfortunately, the file `cars_test_annos.mat` did not contain class labels for each of its entries. This meant that the images in the `cars_test` folder could not be used to train or test our neural network, because the correct class labels would have to be determined by hand. However, there was enough data in just the training dataset that we could set some aside to be used for testing later on. This process is described in detail in the data partitioning section. \n",
    "\n",
    "### Description of car files\n",
    "\n",
    "\n",
    "   The `cars_train` folder contains ~8000 jpg images of cars ranging from the early 1990’s to the early 2010’s. They come from many different manufacturers and have a full range of body styles represented. Unlike many machine learning datasets out there, the images in this file do not have standard size or quality. Some had very small sizes, and others were 10 times the size of the smallest images. Finding a common size to work as input for the neural network would prove to be a challenge later on. However, they are all the jpg file type. These files store pixel information in three channels of 8 bit integers. The next section will describe how these files are read in and prepared for the neural network. \n",
    "\n",
    "### Reading in jpgs with tensorflow\n",
    "\n",
    "   Methods for reading in and preparing the data are located in the notebook `read_car_data` and written out to file `image_processing.py`. The end goal was to read in the photos into a numpy format that would work as valid input to the neural network code supplied in class. The first step in this process was to figure out a way to read the files in. While many libraries for reading images exist, I went with Tensorflow because it has many useful ways of manipulating images that I had become familiar with already, and was readily convertible to numpy in the 2.0 version. The alpha 2.0 version was selected for a few reasons. Mainly, its uses eager execution by default, making debugging much easier and more similar to the numpy style of arrays I am used to. \n",
    "\n",
    "   First, I read each file in the `car_train` folder into a raw tensor. The tensorflow function `tf.io.decode_jpeg` can read a raw tensor from a file into a tf tensor preserving the jpg format. By calling `.numpy()` on these images, I was able to verify they had been read in successfully with `plt.imshow()`. This is where I encountered my first problem. I was running the GPU distribution of tensorflow due to its speed advantages during training. However, reading in over 6500 images for the testing set greatly exceeded the capacity of my GPU. Even with 8GB of memory, the image set could not fit all at once. To work with this limitation, the images had to be split up into batches for preprocessing. This meant that the test and training partitioning had to occur first, and with a set seed so that batches could be consistent if a failure occurred during the processing. After partitioning, batches could be made with the selected images for each set. The function `make_batches` takes the `X` and `T` matrices and a batch size and returns lists containing the batched indices. After this, only one more helper function was needed to read all of the images in. The function `matlab_to_dict` takes the metadata in the matlab format, and changes it into a much more usable dictionary format that would aid in processing later on. \n",
    "At this point, all of the tensors can be read, as long as one batch is processed at a time. The first step of the preparation was to crop the images to the bounding boxes provided in the metadata files. These crop out the extra information in the picture to leave just the relevant part containing the labeled car. This is an important step because some of the pictures contained multiple vehicles or large scenes. The function `crop` can crop a tf.tensor to the bounding box provided in the metadata using the `tf.image.crop_to_bounding_box` function. This function is highly efficient and can run on the GPU, making large scale cropping of images a quick procedure. \n",
    "   \n",
    "   Second, I went through and found the largest and smallest cropped images so I had an idea of the range of image sizes. The largest was around 3000x4000, and the smallest around 50 by 100. This was quite the gap in both size and aspect ratio. At this point, resizing would be required for the majority of images. While a network with only convolutional layers can sometimes handle variable input size, a network with a fixed sized fully connected layer at the end of several convolutional layers needs to have a fixed input size. Because of this, I set out to find the best aspect ratio to resize the images to. My goal was to minimize the padded area of the image. While some sites argued for stretching images to fit a desired size and not using padding, I believed this could cause issues identifying some of the important traits that help to identify a car, especially its body type. A BMW X3 SUV, for example, looks almost the same as a vertically stretched BMW 3 series sedan. To find the best aspect ratio, I found the mean and median of the heights and widths of all the tensors. I then chose the size 275x550 to resize to. It was between the mean and median sizes, indicating that it was a roughly representative size image. I considered using the mode of the ratios, but considering the how variable the sizes were I did not believe it would be a good representation. After this, all images could be resized to 275x550 using the function `tf.image.resize_with_pad`. This function can upsample and downsample images to the desired size while preserving the original aspect ratio. The empty space is padded with zeros. In the future, I would consider using random noise instead, to ensure the model is not learning to identify classes based on the size of the padded area. \n",
    "    After cropping and resizing each batch of photos, the batch was written to the disk using the following process. First, an empty numpy array of the dimensions (Batch Size, Height, Width, 3) was created to hold the entries. Then, a loop called `.numpy()` on each tensor to bring the numpy representation of the image off of the GPU and store it in the precreated array. This resulting array was about 450MB in size. After it was all in memory, this batch was written to a file using the python package `pickle`. This was done for each batch, leaving 7 batches for training and 2 for testing. At this point any of the `.pkl` files could be used for training any neural network that accepts a numpy array.\n",
    "When I started training, I realized that the size 275x550 was too large to easily use convolutional neural nets on the GPU. Even basic structures would not fit on my 8GB GPU. To alleviate this issue, I reran the image processing at ⅗ the original size and ⅕ the original size. The ⅕ size was especially convenient, because at this size the images could feasibly fit in one array, so no batching was necessary. \n",
    "\n",
    "## Reading Vehicle Data and Partitioning - Ryan\n",
    "\n",
    "### Using the vehicle data\n",
    "\n",
    "The data we chose to use contained around 8000 images of cars, complete with information about the images like image size, aspect ratios, etc. These details are discussed further in the [image processing](#Image-Processing) section of this notebook. Additionally, the data contained the classification classes for each image in a metadata file. The format for the classes was a string with the make, model, body style and year. For example, the data for a 2012 Ford Focus would look like the following:\n",
    "\t\n",
    "    Ford Focus Sedan 2012\n",
    "\n",
    "The data was slightly tricky to use because it was in matlab format, as described in the Description of Metada section. Fortunately for us, the scipy.io package has a handy method that reads matlab files into numpy format. The following lines were used to read in the pertinent data:\n",
    "    \n",
    "    meta = scipy.io.loadmat(meta_data_directory + 'cars_meta.mat')\n",
    "    train_annotations = scipy.io.loadmat(meta_data_directory + 'cars_train_annos.mat')\n",
    "    \n",
    "`meta` contains the class names for each image and train_annotations contains the information about each image like image size, aspect ratios, etc. It also contains the index for this specific sample in the `meta` array to get the samle’s class. To get the correct class for a specific sample, this would have to be used:\n",
    "\n",
    "    meta['class_names'][0, 0][0]\n",
    "\n",
    "To get the information about a specific sample, like image size, aspect ratio, etc. the following is used:\n",
    "\t\n",
    "    train_annotations['annotations'][0, 0]\n",
    "\n",
    "This is another example, retrieving the class index for the third image.\n",
    "\t\n",
    "    train_annotations['annotations'][0, 2][4][0][0] \n",
    "In the above example, the 2 refers to 3rd image, and the 4 refers to class index, which is this vehicle’s class location in the metadata file. For example, the above piece of code returns 91, signifying that this sample’s class is located at index 91 in `meta`. The following line of code can then be used to retrieve the class for this sample.\n",
    "\t\n",
    "    meta['class_names'][0, 91][0]\n",
    "    \n",
    "\n",
    "### Creating the T matrix\n",
    "\n",
    "As you can see, the format for this data is decently strange, so work still needed to be done to create the X and T matrices. Since the order of the images in `train_annotations` does not correspond to the order in `meta`, in order to create the T matrix, the target values (class names) must be organized so they correspond to the order of the training values. This wasn’t very difficult, as just a for loop was used to achieve the correct order for the T matrix. The T matrix was also given three columns, one for the make, one for the body style and one for the year. To achieve this, the initial string giving the class had to parsed, separating out the three desired classes and removing the part specifying the model. Getting the make and year was trivial, as the first word in the string was always the make and the last was always the year. The body style, however, was more tricky to retrieve. The body type was usually the second to last word in the string but not always. At times, for example, another identifier denoting something else came after the body style, like in the vehicle `Infinity G Coupe IPL 2012`. We made the decision to always have the body style be the second to last word in the string, so, for cases like this, the body style, in this example `coupe`, replaces the `IPL` resulting in the string `Infinity G Couple 2012`.  In other instances, a body style wasn’t specified at all like for `Buick Regal GS 2008` and `Chevrolet Cobalt SS 2010`. In these cases, we looked up the vehicle online to determine its body style and added that style to the string as the second to last word. Subsequently, these two examples were changed to `Buick Regal GS Sedan 2008` and `Chevrolet Cobalt SS Sedan 2010`. We ended up identifying 16 special cases, and, once identified, they were not very difficult to fix to meet our criteria. Trucks, however, presented another wild card for body style in this data as there are different kinds of trucks represented. For example, some are crew cab, some are extended cab, some are regular/standard cab, some are double cab, etc. Luckily, all truck types end with the word `cab` so we decided to lump all these different truck types together into the body style `truck`.\n",
    "\n",
    "### Partitioning into training and testing sets\n",
    "\n",
    "After configuring the T matrix, the data needed to be partitioned into training and testing sets. The matlab data did have a test annotations file with test data; however, there was no correct class name given for each of these test images so there was no way to determine if the neural network was classifying these correctly. As a result, we decided to partition the training data given to us, since that would still leave plenty of images for training. This partitioning was done in the same way as in the previous assignments, 80% was used as training and 20% for testing.\n",
    "\n",
    "All of this code for the data reading, T matrix creation and partioning into training and testing sets can be seen in the data_management.py file on github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "### Template and Body styles - Ben\n",
    "\n",
    "   Initially, I started training on my desktop with the full size images. I quickly found that my GPU would not be large enough for this task. Of the 8GB, about 2 were taken up by the OS, and another 2 were taken up from the data being read in, leaving only 4 for the neural network. To have more resources, training was moved onto colab from this point onwards. In a GPU instance, colab grants the user access to a Tesla T4 GPU, with 15GB of memory. The whole GPU can be used as well because there is no graphical overhead in a colab instance. This also made it easier to share notebooks so the whole team could try out different networks for each of their tasks. The notebook `final_training_body_style.ipynb` was used as a template for the other training notebooks, `final_training_make.ipynb` and `final_training_decade.ipynb`. First, the notebook imports all the needed packages like in any other notebook. It also installs tensorflow 2.0 because colab by default comes with 1.13. Then, using the package `google.colab` the user can mount their Google drive account as a directory in colab. This was very helpful for using the pickled files created earlier, as well as storing trained networks. \n",
    "\tNext, the file imports `neuralnetworks_pytorch`. This is the file provided in class, with certain modifications. This package presumes that the user is using a square input if they are running a convolution. However, our input is rectangular, so the file had to be modified to work correctly. We could have cropped images to be square input, however this would result in more unnecessary padding. To achieve this the parameter `input_height_width` is split into two parameters, `input_height` and `input_width`. Then, when computing the size of the convolutional networks, `input_height` and `input_width` are updated separately by subtracting the window and dividing by the stride and adding one.  Finally, the `n_inputs` for the fully connected network is found by taking the quantity ` input_height * input_width * n_inputs` instead of `input_height_width ** 2 * n_inputs`. After these changes, the file could run rectangular convolutions just fine.\n",
    "\tThe previously pickled files can be read in using the pickle package into batch files. These files are ready to use numpy files of the dimensions (Batch, Height, Width, Channels). This is called channels last notation, which is normally used by tensorflow. Pytorch uses the channels first notation, so some adjustment is needed to use the neural network package. The function `np.rollaxis` allows the user to change channels last to channels first, resulting in the dimensions (Batch, Channels, Height, Width). \n",
    "\tIn this example, the ⅕ dataset is used, so the data is read in all at once. The unique names of makes, body styles and years are found with the function `np.unique`. Once these unique values are in lists, the target arrays `Tbatch` and `Tbatch_test` can be transformed to use the index of these values instead of the values themselves because the neural net package expects to get ints as class labels. The output class also needs to be selected. Some neural network structures allow for two dimensional classification output. This would have allowed us to train the network to classify on all three target classes at once (Decade, body style and manufacturer). However, the neural network package only supports 1D output, so one of these classes needs to be selected for the target array for training. After it is selected, `n_outputs` can be found with the unique number of outputs in the target array. `input_size_in_pixels` is set to three because the input has three channels. \n",
    "\tNext, various different neural network structures were tested and evaluated on their accuracy. Initially, I found the largest gains in higher n_hiddens counts. So I pushed this value as high as it could go while still fitting on the GPU. At this point, colab will give a warning about high GPU memory usage, but it will not run out of memory during training. Then I tried other hyper parameters. The best benefit came from changing learning rate and n_iterations in tandem. Starting at a .001 rate, I decreased it down in steps to .00005, where I had the most success with body style classification. This necessitated an increase in n_iterations at the same time. Lower n_iterations were tried, but the classification accuracy went down, so I stopped at .00005. The graph shows the performance of the network as `n_iterations` varies. Batch size is also set at 200 based on memory limitations. Larger batches can speed up training, but to large of a batch can not fit on the GPU all at once. The strides and window sizes are chosen based on using odd kernel sizes and strides of 2. This helps to reduce the size of the network in memory and increase performance. When testing where to allocate hidden units, I found that increasing the units per convolution had some benefit, but increasing the number in the fully connected layers had the most benefit. \n",
    "\tAfter finding a satisfying result, the network is written out to a file using `pickle`. These files are quite large at ~350 MB. These files are loaded up by the server. Using `nnet.use()` the output of `image_to_numpy()` can be used to make predictions of any photos the user gives as input.\n",
    "    \n",
    "### Vehicle Make - Ryan\n",
    "\n",
    "Training for make used the same template as described above and can be seen in the `final_training_make.ipynb`. As discussed in the [image processing](#Image-Processing) section, the image files were edited reducing the resolution in order to save memory. There were three different image sets available to use, depending on which classification desired, one fifth the normal size, three-fifths the normal size and regular image sizes. For vehicle make, the three-fifths images worked the best. Because the logo is best way to identify the vehicle make, the resolution on the one-fifth images was too low for the neural nets to distinguish even though it could use complex hidden layer structures. Out of various combinations of `n_iterations`, `learning_rate`, `batch_size`, `windows`, `strides` and `n_hiddens_by_layer`, the best percentage correctly classified for the test data was around six percent. Conversely, for the normal images, the memory capabilities weren’t strong enough to use complex neural nets as the GPU ran out of memory with only 50 hidden units. As a result, the best percentage correctly classified for the test data for different combinations of parameters was about 3%. This is obviously not where we want it to be, so the three-fifths images are the best ones to go with.\n",
    "\n",
    "As discussed earlier, the three-fifths images had to be separated into seven batches of 1000 images each since using them all at once takes up too much memory. Therefore, because the neural network objects keep their weights after training, each batch is trained, then the next batch is loaded in and trained with the same neural network object. This can all be seen in the `final_training_make.ipynb` notebook. Unfortunately, the three-fifths images are still big enough to where complex neural networks use too much memory, but they are still much better than the normal image sizes. After various runs through many different parameters the best hidden layer structure found that didn’t exceed the memory was [50, 65, 80, 100, 100] with `windows` of [27, 19, 9, 9] and `strides` of [2, 2, 2, 2]. It seems that having the 4 convolutional layers with increases hidden units and decreasing window sizes, followed by a large fully connected layer, had the most success classifying the data. Several other combinations of these were tried but this combination seems to have the most success. This makes sense since the large fully connected layer can tie everything back together after the convolutional layers learn all they can. Additionally, `batch_size` of 25 and `learning_rate` of 0.0001 were used. This low `learning_rate` proved very helpful in training, bumping up the accuracy by a couple percentage points compared to the original `learning_rate` of 0.01.  A very low `n_iterations` of 5 was used because the accuracy went down with any more iterations. This could possibly be due to overtraining of the data. The final combination of parameters is as follows:\n",
    "\n",
    "* `n_hiddens_by_layer` = [50, 65, 80, 100, 100]\n",
    "* `windows` = [27, 19, 9, 9] \n",
    "* `strides` = [2, 2, 2, 2]\n",
    "* `batch_size` = 25 \n",
    "* `learning_rate` = 0.0001\n",
    "* `n_iterations` = 5\n",
    "\n",
    "This combination correctly classifies close to 11% of the test data samples correctly. This may seem a bit low but considering there were 49 different makes represented in the data and the random chance odds of correctly classifying them is about 2%, this isn't too bad.\n",
    "\n",
    "### Vehicle Decade - Chris\n",
    "Just like body style, we used the same [image processing](#Image-Processing) described earlier. We used the smaller image size yet again to ensure that the network could be as large as possible. Right from the start, even with slightly small networks, the accuracy was usually 66.24% from the start, using 4 convolutional windows of [7, 5, 3, 3] and strides of 2. Using higher iterations generally would only decrease the accuracy, likely do to overtraining. We found that getting the accuracy above 66.24% was very difficult and one iteration would have produced a network slightly worse than the one we ended up using, with only 0.5% of a decrease in accuracy. Higher batch sizes tended to work better in later iterations. However, moving to a much larger network did increase the accuracy slightly, but not as much as we would have hoped for. Higher learning rates often worked very poorly, and testing accuracy was usually 10% lower at 10 iterations. But the best accuracies were often at 5-7 iterations so we chose 6 iterations for this network. After playing with parameters and 30 or so neural networks, we found the best parameters to be:\n",
    "* `n_hiddens_by_layer` = [300, 400, 500, 800, 5000, 100000]\n",
    "* `windows` = [7, 5, 3, 3] \n",
    "* `strides` = [2, 2, 2, 2]\n",
    "* `batch_size` = 200\n",
    "* `learning_rate` = 0.00001\n",
    "* `n_iterations` = 6\n",
    "\n",
    "The final testing accuracy was 66.73%, and was a lot better than random chance at 33% for the three decades represented in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final testing accuracies were achieved as body style classifications at 47%, make classifications at 11%, and decade classifications at 67%. With the web application, we could use our phones running the web app to take pictures and see how we were doing. They were often times slightly off, like Chris's 2006 Nissan Altima classified as a 2010's coupe or convertible, or Ryan's 2014 F150 classified as 2010's SUV. However, comparing to Google Lens, we were not too far off from achieving similar accuracies, as Ryan's truck was wrong but the Chris's sedan was correct. For images we found on the internet, we had some interesting results, such as a brand new Camaro classified correctly as a 2010 Chevrolet, but Jeep's classified as 2010's Fiat coupes. Additionally, we also tried some vehicles that were not represented in the data like a tank. The neural net guessed it was Ford coupe, so it may need a little bit more training there. Screenshots for other attempted classifications can be seen in the screenshots folder in the submitted .zip file. All in all it was challenging, and we were glad to see that it was oftentimes closer to the mark than the accuracies could display.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned a lot during this project about a variety of topics. We got much more comfortable with neural networks and as a result were able to use them more effectively. With this increased knowledge we were able to choose parameter values more carefully and knew how to change them to increase the neural network’s effectiveness. We also learned a lot about image processing and how to use neural networks with images. Furthermore, we also learned how to use the pickle package to serialize objects, like we did with the trained neural networks that the server for the website used when attempting to classify images. This presented a challenge initially because we trained the networks on a GPU but the server used them on a CPU. After some trial and error this was solved by simply moving the neural network off the GPU and to the CPU before serializing them for use on the server. In terms of changes we needed to make, we ended up not classifying price like we initially planned because the data did not have any information about price. In addition, we decided not to pass the features it was classified based off of back to the frontend for the user due to time constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [A. Pandey], “File Upload with React & Flask,” Ashish Pandey, 06-Jul-2018. https://medium.com/@ashishpandey_1612/file-upload-with-react-flask-e115e6f2bf99.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Your report for a single person team should contain approximately 2,000 to 5,000 words, in markdown cells.  You can count words by running the following python code in your report directory.  Projects with two people, for example, should contain 4,000 to 8,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file Haynes_Newell_Williams-TermProject.ipynb is 6206\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from nbformat import current\n",
    "import glob\n",
    "nbfile = glob.glob('Haynes_Newell_Williams-TermProject.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
